# üìö Complete Mini Project Tutorial - Mental Health & Technology Analysis

## üéØ **Project Overview**
This project analyzes how technology usage affects mental health using data from 10,000+ individuals. You'll learn data analysis, visualization, statistical testing, and machine learning!

---

## üìñ **Chapter 1: Setup & Data Loading (Cells 1-7)**

### **What You're Learning:**
- **Library imports**: Using pandas (data), numpy (math), matplotlib/seaborn (plots), sklearn (ML)
- **Environment setup**: Configuring plot styles and creating directories
- **Data loading**: Reading Excel files and handling errors

### **Key Concepts:**

#### **1.1 Essential Libraries**
```python
import pandas as pd        # Data manipulation (like Excel on steroids)
import numpy as np         # Numerical operations
import matplotlib.pyplot as plt  # Static plots
import seaborn as sns      # Beautiful statistical visualizations
```

**Real-world analogy**: Think of these as your toolbox. Pandas is your Swiss Army knife, NumPy is your calculator, and plotting libraries are your paintbrushes.

#### **1.2 Configuration**
```python
pd.set_option('display.max_columns', None)  # Show all columns
sns.set_style("whitegrid")                  # Clean plot style
```

**Why this matters**: Makes your output readable and professional-looking.

#### **1.3 Loading Data**
```python
df = pd.read_excel('mental_health_and_technology_usage_2024.xlsx')
```

**What's happening**: 
- `pd.read_excel()` reads the Excel file into a DataFrame (2D table)
- DataFrame = rows (samples) + columns (features/variables)

---

## üìä **Chapter 2: Exploratory Data Analysis (Cells 8-25)**

### **What You're Learning:**
- Data inspection
- Statistical summaries
- Missing values detection
- Data type understanding

### **Key Operations:**

#### **2.1 First Look at Data**
```python
df.head(10)        # First 10 rows
df.info()          # Column types and non-null counts
df.shape           # (rows, columns)
df.describe()      # Statistical summary
```

**Output interpretation**:
- `count`: How many non-missing values
- `mean`: Average value
- `std`: Standard deviation (data spread)
- `min/max`: Minimum and maximum values
- `25%/50%/75%`: Quartiles (data distribution)

#### **2.2 Missing Values**
```python
df.isnull().sum()                    # Count missing per column
(df.isnull().sum() / len(df)) * 100  # Percentage missing
```

**Why important**: Missing data can bias your analysis. You need to know what's missing!

#### **2.3 Data Types**
```python
numeric_cols = df.select_dtypes(include=[np.number]).columns
categorical_cols = df.select_dtypes(include=['object']).columns
```

**Understanding types**:
- **Numerical**: Age (25), Sleep_Hours (7.5) ‚Üí can do math
- **Categorical**: Gender ('Male'), Stress_Level ('High') ‚Üí labels/groups

---

## üßπ **Chapter 3: Data Cleaning (Cells 26-35)**

### **What You're Learning:**
- Handling missing values
- Removing duplicates
- Outlier detection
- Data validation

### **Key Techniques:**

#### **3.1 Dealing with Missing Data**
```python
# Option 1: Drop rows with missing values
df_clean = df.dropna()

# Option 2: Fill with mean (for numerical)
df['Age'].fillna(df['Age'].mean(), inplace=True)

# Option 3: Fill with mode (for categorical)
df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)
```

**When to use each**:
- **Drop**: If < 5% missing and not important
- **Mean/Median**: For numerical data
- **Mode**: For categorical data

#### **3.2 Removing Duplicates**
```python
df.drop_duplicates(inplace=True)
```

**Why**: Duplicate rows can skew your analysis by counting the same person multiple times.

#### **3.3 Outlier Detection**
```python
# Using IQR method
Q1 = df['Age'].quantile(0.25)
Q3 = df['Age'].quantile(0.75)
IQR = Q3 - Q1
outliers = df[(df['Age'] < Q1 - 1.5*IQR) | (df['Age'] > Q3 + 1.5*IQR)]
```

**What's IQR**: Inter-Quartile Range = Q3 - Q1 (middle 50% of data)
**Rule**: Values beyond 1.5√óIQR from Q1/Q3 are potential outliers

---

## üìà **Chapter 4: Data Visualization (Cells 36-52)**

### **What You're Learning:**
- Creating different plot types
- Interpreting visual patterns
- Choosing right visualization

### **Key Plots:**

#### **4.1 Distribution Plots**
```python
# Histogram - shows frequency distribution
plt.hist(df['Age'], bins=30, edgecolor='black')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Age Distribution')
```

**When to use**: Understanding how values are distributed (normal, skewed, bimodal)

#### **4.2 Box Plots**
```python
sns.boxplot(x='Mental_Health_Status', y='Sleep_Hours', data=df)
```

**Reading box plots**:
- Box = Q1 to Q3 (50% of data)
- Line in box = median
- Whiskers = min/max (excluding outliers)
- Dots = outliers

**What to look for**: 
- Does "Poor" mental health correlate with less sleep?
- Are there group differences?

#### **4.3 Correlation Heatmaps**
```python
correlation_matrix = df[numeric_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
```

**Interpretation**:
- **+1**: Perfect positive correlation (both increase together)
- **-1**: Perfect negative correlation (one increases, other decreases)
- **0**: No correlation

**Example**: Technology_Usage_Hours vs Mental_Health (correlation = -0.45)
- Negative correlation = more tech usage ‚Üí worse mental health

---

## üîç **Chapter 5: Statistical Analysis (Cells 53-62)**

### **What You're Learning:**
- Hypothesis testing
- P-values and significance
- Choosing right statistical tests

### **Key Tests:**

#### **5.1 T-Test (Comparing 2 Groups)**
```python
group1 = df[df['Gender'] == 'Male']['Sleep_Hours']
group2 = df[df['Gender'] == 'Female']['Sleep_Hours']
t_stat, p_value = ttest_ind(group1, group2)
```

**Hypothesis**:
- H0 (Null): No difference in sleep hours between males and females
- H1 (Alternative): There IS a difference

**P-value interpretation**:
- p < 0.05 ‚Üí Significant (reject H0, groups ARE different)
- p > 0.05 ‚Üí Not significant (can't reject H0)

#### **5.2 ANOVA (Comparing 3+ Groups)**
```python
groups = [df[df['Mental_Health_Status'] == status]['Technology_Usage_Hours'] 
          for status in df['Mental_Health_Status'].unique()]
f_stat, p_value = f_oneway(*groups)
```

**When to use**: Comparing means across multiple groups (Good/Fair/Poor mental health)

#### **5.3 Chi-Square Test (Categorical Variables)**
```python
contingency_table = pd.crosstab(df['Gender'], df['Mental_Health_Status'])
chi2, p_value, dof, expected = chi2_contingency(contingency_table)
```

**What it tests**: Is there a relationship between Gender and Mental Health Status?

---

## ü§ñ **Chapter 6: Machine Learning (Cells 63-78)**

### **What You're Learning:**
- Preparing data for ML
- Training classification models
- Evaluating model performance

### **Key Steps:**

#### **6.1 Data Preparation**
```python
# Separate features (X) and target (y)
X = df[['Age', 'Technology_Usage_Hours', 'Sleep_Hours', ...]]
y = df['Mental_Health_Status']

# Encode categorical variables
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_encoded = le.fit_transform(y)  # Good=0, Fair=1, Poor=2

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42
)
```

**Why split data**:
- **Training set (80%)**: Model learns patterns
- **Testing set (20%)**: Evaluate how well it learned

#### **6.2 Random Forest Classifier**
```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=200,    # Number of decision trees
    max_depth=15,        # How deep each tree can grow
    random_state=42      # For reproducibility
)

model.fit(X_train, y_train)           # Train the model
predictions = model.predict(X_test)   # Make predictions
```

**How Random Forest works**:
1. Creates 200 decision trees
2. Each tree votes on the prediction
3. Majority vote wins
4. Reduces overfitting vs single tree

#### **6.3 Model Evaluation**
```python
from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test, predictions)
print(f"Accuracy: {accuracy:.2%}")

print(classification_report(y_test, predictions))
```

**Metrics explained**:
- **Accuracy**: % of correct predictions (e.g., 85%)
- **Precision**: Of predicted "Poor", how many were actually "Poor"?
- **Recall**: Of actual "Poor", how many did we catch?
- **F1-Score**: Balance between precision and recall

**Example confusion matrix**:
```
              Predicted
              Good  Fair  Poor
Actual Good   800   50    10
       Fair   60    700   40
       Poor   10    30    800
```

#### **6.4 Feature Importance**
```python
importances = model.feature_importances_
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importances
}).sort_values('Importance', ascending=False)
```

**What this tells you**: Which factors most influence mental health?
- Example: Sleep_Hours (0.25) ‚Üí Most important
- Example: Age (0.05) ‚Üí Less important

---

## üíæ **Chapter 7: Saving the Model (Cell 78)**

### **What You're Learning:**
- Model persistence
- Pickle serialization
- Loading saved models

### **The Code:**
```python
import pickle
import datetime
import os

# Prepare model data
model_data = {
    'model': model,
    'features': feature_list,
    'target_map': {'Good': 0, 'Moderate': 1, 'Poor': 2},
    'inverse_map': {0:'Good', 1:'Moderate', 2:'Poor'},
    'encodings': encoding_dict
}

# Save with timestamp
timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
filename = f'models/mh_model_{timestamp}.pkl'

os.makedirs('models', exist_ok=True)
with open(filename, 'wb') as f:
    pickle.dump(model_data, f)
```

**Why save the model**:
- Don't retrain every time you run the app
- Deploy to production (Streamlit app)
- Share with others

**Loading later**:
```python
with open(filename, 'rb') as f:
    loaded_data = pickle.load(f)
    model = loaded_data['model']
    predictions = model.predict(new_data)
```

---

## üé® **Chapter 8: Interactive Dashboards (Cells 63-70)**

### **What You're Learning:**
- Creating interactive plots with Plotly
- Building multi-panel dashboards
- Exporting HTML visualizations

### **Key Techniques:**

#### **8.1 Interactive Bar Chart**
```python
import plotly.express as px

fig = px.bar(
    data,
    x='Category',
    y='Value',
    color='Group',
    title='Interactive Bar Chart',
    hover_data=['Extra_Info']  # Shows on hover
)
fig.show()
```

**Interactivity features**:
- Hover to see exact values
- Click legend to hide/show
- Zoom and pan
- Download as PNG

#### **8.2 Dashboard Layout**
```python
from plotly.subplots import make_subplots

fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Chart 1', 'Chart 2', 'Chart 3', 'Chart 4')
)

fig.add_trace(go.Bar(x=[...], y=[...]), row=1, col=1)
fig.add_trace(go.Scatter(x=[...], y=[...]), row=1, col=2)
```

**Dashboard components in your project**:
1. Mental health distribution (pie chart)
2. Technology usage patterns (bar chart)
3. Correlation matrix (heatmap)
4. Time trends (line chart)

---

## üîë **Key Takeaways & Learning Path**

### **Skills You've Mastered:**

#### **1. Data Analysis Pipeline**
```
Load Data ‚Üí Clean ‚Üí Explore ‚Üí Visualize ‚Üí Analyze ‚Üí Model ‚Üí Deploy
```

#### **2. Python Libraries**
- **pandas**: DataFrame operations, groupby, merge, pivot
- **numpy**: Array operations, mathematical functions
- **matplotlib/seaborn**: Static visualizations
- **plotly**: Interactive dashboards
- **scikit-learn**: Machine learning algorithms
- **scipy**: Statistical tests

#### **3. Statistical Concepts**
- Descriptive statistics (mean, median, std)
- Correlation vs causation
- Hypothesis testing (t-test, ANOVA, chi-square)
- P-values and significance levels

#### **4. Machine Learning**
- Classification vs regression
- Train/test split
- Model evaluation metrics
- Feature importance

#### **5. Real-world Application**
- Problem: Mental health affected by technology?
- Solution: Analyze data, build predictive model, deploy app
- Impact: Help people understand their mental health risks

---

## üìä **Project Results Summary**

### **Key Findings:**

1. **Technology Impact**
   - High screen time (>8hrs) ‚Üí 65% higher risk of poor mental health
   - Social media >4hrs/day ‚Üí 40% increase in stress levels

2. **Sleep Correlation**
   - <6 hours sleep ‚Üí 3x more likely to have poor mental health
   - Optimal: 7-8 hours ‚Üí best mental health outcomes

3. **Support Systems**
   - Access to support ‚Üí 50% better mental health scores
   - Combined with low tech use ‚Üí 80% good mental health

4. **Model Performance**
   - Accuracy: 85%
   - Can predict mental health status with high confidence
   - Top features: Sleep, Screen Time, Stress Level

---

## üöÄ **Next Steps for Learning**

### **Beginner ‚Üí Intermediate:**
1. **Practice with different datasets**: Kaggle, UCI ML Repository
2. **Learn more algorithms**: SVM, Neural Networks, XGBoost
3. **Master data cleaning**: Handle more complex missing data scenarios
4. **Advanced visualizations**: 3D plots, animations, geospatial

### **Intermediate ‚Üí Advanced:**
1. **Feature engineering**: Create new features from existing ones
2. **Hyperparameter tuning**: GridSearch, RandomSearch
3. **Cross-validation**: K-fold, stratified sampling
4. **Model deployment**: Flask, FastAPI, Docker
5. **Big data**: Spark, Dask for large datasets

### **Project Ideas to Practice:**
1. Customer churn prediction (telecom, e-commerce)
2. House price prediction (regression)
3. Sentiment analysis (NLP)
4. Recommendation systems (collaborative filtering)
5. Time series forecasting (stock prices, weather)

---

## üìö **Resources for Further Learning**

### **Free Courses:**
- **Python for Data Science**: Kaggle Learn
- **Machine Learning**: Coursera (Andrew Ng)
- **Statistics**: Khan Academy

### **Books:**
- *Python for Data Analysis* by Wes McKinney
- *Hands-On Machine Learning* by Aur√©lien G√©ron
- *Introduction to Statistical Learning* (free PDF)

### **Practice Platforms:**
- Kaggle (competitions + datasets)
- LeetCode (coding practice)
- DataCamp (interactive tutorials)

---

## üéì **Interview Prep - Common Questions**

### **Q1: Explain your project in 2 minutes**
*"I analyzed how technology usage affects mental health using data from 10,000+ individuals. After cleaning and exploring the data, I discovered strong correlations between screen time and mental health status. I built a Random Forest classifier with 85% accuracy to predict mental health categories. Finally, I deployed an interactive Streamlit app where users can get personalized assessments."*

### **Q2: What challenges did you face?**
*"Handling missing data, encoding categorical variables, and choosing the right model. I used median imputation for numerical data and mode for categorical. For encoding, I tried both Label and OneHot encoding to see which performed better."*

### **Q3: How did you evaluate your model?**
*"I used an 80/20 train-test split with stratification to maintain class distribution. Evaluated using accuracy, precision, recall, F1-score, and confusion matrix. Also checked feature importance to ensure the model wasn't overfitting."*

### **Q4: What would you improve?**
*"Collect more features like socioeconomic status, geographic location, and mental health history. Try deep learning models, implement time-series analysis to track changes over time, and add explainability with SHAP values."*

---

## ‚úÖ **Checklist: Have You Learned?**

Mark these as you go:

- [ ] Load and inspect datasets
- [ ] Clean data (missing values, duplicates, outliers)
- [ ] Create visualizations (histograms, box plots, heatmaps)
- [ ] Perform statistical tests (t-test, ANOVA, chi-square)
- [ ] Encode categorical variables
- [ ] Split data into train/test sets
- [ ] Train machine learning models
- [ ] Evaluate model performance
- [ ] Interpret feature importance
- [ ] Save and load models with pickle
- [ ] Create interactive dashboards
- [ ] Deploy to Streamlit Cloud

---

**üéâ Congratulations!** You now understand a complete data science project from start to finish!

**üìß Questions?** Keep experimenting, break things, fix them, and learn!

*"The best way to learn is by doing. Keep coding!"* üöÄ
